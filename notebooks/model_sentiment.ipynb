{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: distilbert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rating\n",
    "# 5    33373\n",
    "# 4     6304\n",
    "# 1     4638\n",
    "# 3     3432\n",
    "# 2     2035\n",
    "# Name: count, dtype: int64\n",
    "\n",
    "# The dataset shows significant class imbalance. \n",
    "# To address this, applied upsampling/downsampling techniques. \n",
    "# Using DistilBERT with REGRESSION for the classification task \n",
    "# since labels (1-5) map to outcomes: 1-3 -> Negative, Neutral, Positive sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and balance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/reviews_cleaned.csv')\n",
    "\n",
    "data = data[['text', 'rating']].rename(columns = {'rating': 'labels'})\n",
    "\n",
    "data = data[~data['text'].isna()]\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNsampling class 'Rating 5'\n",
    "class_5 = data[data['labels'] == 5]\n",
    "class_5 = class_5.sample(n = 6304, random_state = 42)\n",
    "\n",
    "# Keep 7,242 rows for class 'Rating 4' \n",
    "class_4 = data[data['labels'] == 4]\n",
    "\n",
    "# UPsampling class 'Rating 3'\n",
    "class_3 = data[data['labels'] == 3]\n",
    "class_3 = class_3.sample(n = 6304, replace = True, random_state = 42)\n",
    "\n",
    "# UPsampling class 'Rating 2'\n",
    "class_2 = data[data['labels'] == 2]\n",
    "class_2 = class_2.sample(n = 6304, replace = True, random_state = 42)\n",
    "\n",
    "# UPsampling class 'Rating 1'\n",
    "class_1 = data[data['labels'] == 1]\n",
    "class_1 = class_1.sample(n = 6304, replace = True, random_state = 42)\n",
    "\n",
    "data = pd.concat([class_1, class_2, class_3, class_4, class_5])\n",
    "\n",
    "# Shuffle combined rows\n",
    "data = data.sample(frac=1, random_state = 42).reset_index(drop = True)\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "\n",
    "# Load TinyBERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding = 'max_length', truncation = True, max_length = 128)\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched = True)\n",
    "test_dataset = test_dataset.map(tokenize, batched = True)\n",
    "\n",
    "# Normalize column 'labels'\n",
    "train_dataset = train_dataset.map(lambda x: {'labels': x['labels'] / 5})\n",
    "test_dataset = test_dataset.map(lambda x: {'labels': x['labels'] / 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TinyBERT model\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('huawei-noah/TinyBERT_General_4L_312D', num_labels = 1)  # Regression task\n",
    "\n",
    "model = model.to(device)  # Move model to GPU if available\n",
    "\n",
    "print(f\"Model is on: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Trainer with MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Trainer to use MSE loss for regression\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs = False, **kwargs):\n",
    "        labels = inputs.pop('labels').float()  # Extract labels\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits.squeeze()\n",
    "\n",
    "        # Compute MSE Loss\n",
    "        loss_fn = nn.MSELoss()\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, truth = eval_pred\n",
    "\n",
    "    # Define bins for regression 0-1\n",
    "    bins = np.linspace(1, 5, 6)\n",
    "    predictions = np.digitize(predictions, bins) - 1\n",
    "    truth = np.digitize(truth, bins) - 1\n",
    "    \n",
    "    # Compute metrics\n",
    "    mse = mean_squared_error(truth, predictions)\n",
    "    mae = mean_absolute_error(truth, predictions)\n",
    "    acc = accuracy_score(truth, predictions)\n",
    "    \n",
    "    return {'mse': mse, 'mae': mae, 'accuracy': acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = '../models/sentiment/results',\n",
    "    eval_strategy = 'epoch',\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    num_train_epochs = 3,\n",
    "    max_grad_norm = 1.0,  # Gradient clipping\n",
    "    learning_rate = 1e-5,  # 3e-6\n",
    "    weight_decay = 0.01,\n",
    "    logging_steps = 500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    processing_class = tokenizer,\n",
    "    compute_metrics = compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './../models/sentiment'\n",
    "\n",
    "# model.save_pretrained(output_dir)\n",
    "# tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../models/sentiment'\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded and moved to device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Sentiment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(texts, model, tokenizer):\n",
    "\n",
    "    # Tokenize the input texts\n",
    "    inputs = tokenizer(texts, return_tensors = 'pt', padding = True, truncation = True, max_length = 128)\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "\t# Perform inference\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits.squeeze()\n",
    "\n",
    "    return logits.cpu().tolist() if logits.ndim > 0 else [logits.item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"This game is a total disappointment. The controls are clunky, the graphics feel outdated, and the story is bland and uninspired. Definitely not worth the price.\",  # Negative\n",
    "    \"The game is okay. It has some fun moments, but nothing stands out. The graphics are decent, and the gameplay is smooth, but it lacks depth and replay value.\",  # Neutral\n",
    "    \"Absolutely loved this game! The story is engaging, the characters are well-developed, and the visuals are stunning. The gameplay is smooth and immersive—definitely a masterpiece!\"  # Positive\n",
    "]\n",
    "\n",
    "ratings = predict_sentiment(examples, model, tokenizer)\n",
    "\n",
    "# Print results\n",
    "for text, rating in zip(examples, ratings):\n",
    "    print(f\"Review: {text}\\nPredicted Rating: {rating:.2f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
